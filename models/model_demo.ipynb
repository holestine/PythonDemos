{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Different Model Types\n",
    "This notebook compares the performance of MLP, CNN, ViT and Hybrid models on MNIST, CIFAR10 and CIFAR100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:00:08.901141: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745503208.916202 1236403 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745503208.920805 1236403 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-24 07:00:08.938273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from vit_pytorch import ViT\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from timer import Timer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\" Counts the number of trainable parameters\n",
    "\n",
    "    Args:\n",
    "        model (object): A PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of trainable parameters\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to convert grayscale to RGB\n",
    "class GrayToRGB(object):\n",
    "    def __call__(self, image):\n",
    "        return image.repeat(3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPNet(nn.Module):\n",
    "    \"\"\" Creates a fully connected multilayer perceptron. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, num_classes, channels):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (array): The resolution of the image\n",
    "            num_classes (int): The number of output classes\n",
    "            channels (int): The color depth of the image\n",
    "        \"\"\"\n",
    "        super(MLPNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(image_size * channels, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    \"\"\" Creates a convolutional neural network. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, num_classes, channels, ):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (tuple): The height and width of the image\n",
    "            num_classes (int): The number of output classes\n",
    "            channels (int): The color depth of the image\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CNNNet, self).__init__()\n",
    "\n",
    "        (image_height, image_width) = image_size\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        fc_size = self.forward(torch.rand((1, channels, image_height, image_width)), True)\n",
    "\n",
    "        self.fc1 = nn.Linear(fc_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, get_fc_size=False):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "            get_fc_size (bool): Flag that determines the flattened size of the last convolutional layer\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        if get_fc_size:\n",
    "            return x.nelement()\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTNet(nn.Module): \n",
    "    \"\"\" Creates a vision transformer. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_size, patch_size, num_classes, channels):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (array): The resolution of the image\n",
    "            patch_size (int): The size (patch_size x patch_size) of the patches to extract from the image \n",
    "            num_classes (int): The number of output classes\n",
    "            channels (int): The color depth of the image\n",
    "        \"\"\"\n",
    "        HIDDEN_SIZE = 768\n",
    "        DEPTH = 6\n",
    "        HEADS = 12\n",
    "        MLP_DIM = 3072\n",
    "        \n",
    "        super(ViTNet, self).__init__()\n",
    "        \n",
    "        self.model = ViT(\n",
    "                        image_size = image_size,\n",
    "                        patch_size = patch_size,\n",
    "                        num_classes = num_classes,\n",
    "                        dim = HIDDEN_SIZE,\n",
    "                        depth = DEPTH,\n",
    "                        heads = HEADS,\n",
    "                        mlp_dim = MLP_DIM,\n",
    "                        channels = channels,\n",
    "                        dropout = 0.1,\n",
    "                        emb_dropout = 0.1\n",
    "                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        x = self.model(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridNet(nn.Module):\n",
    "    \"\"\" Creates a CNN connected to a ViT. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, channels, num_classes):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (array): The resolution of the image\n",
    "            channels (int): The color depth of the image\n",
    "            num_classes (int): The number of output classes\n",
    "        \"\"\"\n",
    "        HIDDEN_SIZE = 768\n",
    "        DEPTH = 6\n",
    "        HEADS = 12\n",
    "        MLP_DIM = 3072\n",
    "\n",
    "        super(HybridNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        image_width = int((math.sqrt(image_size) - 4) / 2)\n",
    "\n",
    "        self.vit = ViT(\n",
    "                        image_size = image_width ** 2,\n",
    "                        patch_size = int(image_width / 2),\n",
    "                        num_classes = num_classes,\n",
    "                        dim = HIDDEN_SIZE,\n",
    "                        depth = DEPTH,\n",
    "                        heads = HEADS,\n",
    "                        mlp_dim = MLP_DIM,\n",
    "                        channels = 64,\n",
    "                        dropout = 0.1,\n",
    "                        emb_dropout = 0.1\n",
    "                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.vit(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\" Creates a DenseNet backbone with a new classification head. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, channels, num_classes):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (array): The resolution of the image\n",
    "            channels (int): The color depth of the image\n",
    "            num_classes (int): The number of output classes\n",
    "        \"\"\"\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        self.densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        num_features = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Linear(num_features, num_classes) # replace num_classes with your desired number of output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        #x = self.densenet.features(x)\n",
    "        x = self.densenet(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    \"\"\" Performs a single iteration of training\n",
    "\n",
    "    Args:\n",
    "        model (object): The model to train\n",
    "        device (object): The device that will host the training\n",
    "        train_loader (object): The training dataset\n",
    "        optimizer (object): The optimization algorithm\n",
    "        epoch (int): The current epoch\n",
    "        log_interval (int): The frequency to log data to the screen\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, dataset, model_type, writer, epoch):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (object): The model to test\n",
    "        device (object): The device that will host the testing\n",
    "        test_loader (object): The test dataset\n",
    "        dataset (string): The name of the dataset to use in testing\n",
    "        model_type (string): The type of the model to test\n",
    "        writer (object): Tensorboard logger\n",
    "        epoch (int): The current epoch in training\n",
    "\n",
    "    Returns:\n",
    "        accuracy (): \n",
    "        actual ():\n",
    "        predicted (): \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            with Timer(\"{} {}\".format(model_type, dataset)):\n",
    "                output = model(data)\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            actual.extend(target.cpu().numpy())\n",
    "            predicted.extend([np.argmax(pred) for pred in output.cpu().numpy()])\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    writer.add_scalar('Accuracy/{} {}'.format(model_type, dataset), accuracy, epoch)\n",
    "    return accuracy, actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models and datasets that we'll be evaluating\n",
    "\n",
    "MODEL_TYPES = [\"MLP\", \"CNN\", \"ViT\", \"HYBRID\", \"DenseNet\"]\n",
    "DATASETS    = [\"MNIST\", \"CIFAR10\", \"CIFAR100\"] \n",
    "#MODEL_TYPES = [\"DenseNet\"]\n",
    "#DATASETS    = [\"MNIST\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(batch_size, test_batch_size, lr, gamma, epochs, log_dir, log_interval):\n",
    "    \"\"\" Performs training and testing for all models and datasets specified above \n",
    "\n",
    "    Args:\n",
    "        batch_size (int): The training batch size\n",
    "        test_batch_size (int): The test batch size\n",
    "        lr (float): The learning rate\n",
    "        gamma (float): _description_\n",
    "        epochs (int): The number of epochs of training\n",
    "        log_dir (_type_): The directory for the logs\n",
    "        log_interval (int): How frequently to log to the screen\n",
    "\n",
    "    Returns:\n",
    "        accuracies (dictionary): The model accuracies by model type and dataset\n",
    "        parameters (dictionary): The number of trainable parameters in each model\n",
    "        expected_vs_actual (dictionary): The expected and actual values used in testing each of the models\n",
    "    \"\"\"\n",
    "    # Determine if we should use a GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # Reduce precision to speed up training\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs  = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    accuracies = {}\n",
    "    parameters = {}\n",
    "    expected_vs_actual = {}\n",
    "    \n",
    "    for dataset in DATASETS:\n",
    "        for model_type in MODEL_TYPES:\n",
    "            print(\"Evaluating {} with {}\".format(model_type, dataset))\n",
    "\n",
    "            if dataset == \"MNIST\":\n",
    "                if model_type == \"DenseNet\":\n",
    "                    transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307), (0.3081)),\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    GrayToRGB(),\n",
    "                ])\n",
    "                else:\n",
    "                    transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307), (0.3081))\n",
    "                ])\n",
    "                train_set = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "                test_set = datasets.MNIST('./data', train=False, transform=transform)\n",
    "                image_height = 28\n",
    "                image_width = 28\n",
    "                patch_size = 14\n",
    "                num_classes = 10\n",
    "                channels = 1\n",
    "            elif dataset == \"CIFAR10\":\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                ])\n",
    "                train_set = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "                test_set = datasets.CIFAR10('./data', train=False, transform=transform)\n",
    "                image_height = 32\n",
    "                image_width = 32\n",
    "                patch_size = 16\n",
    "                num_classes = 10\n",
    "                channels = 3\n",
    "            elif dataset == \"CIFAR100\":\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                ])\n",
    "                train_set = datasets.CIFAR100('./data', train=True, download=True, transform=transform)\n",
    "                test_set = datasets.CIFAR100('./data', train=False, transform=transform)\n",
    "                image_height = 32\n",
    "                image_width = 32\n",
    "                patch_size = 16\n",
    "                num_classes = 100\n",
    "                channels = 3\n",
    "                \n",
    "            train_loader = torch.utils.data.DataLoader(train_set, **train_kwargs)\n",
    "            test_loader  = torch.utils.data.DataLoader(test_set, **test_kwargs)\n",
    "\n",
    "            if model_type == \"MLP\":\n",
    "                model = MLPNet(image_height * image_width, num_classes, channels).to(device)\n",
    "            elif model_type == \"CNN\":\n",
    "                model = CNNNet((image_height, image_width), num_classes, channels).to(device)\n",
    "            elif model_type == \"ViT\":\n",
    "                model = ViTNet(image_height * image_width, patch_size, num_classes, channels).to(device)\n",
    "            elif model_type == \"HYBRID\":\n",
    "                model = HybridNet(image_height * image_width, channels, num_classes).to(device)\n",
    "            elif model_type == \"DenseNet\":\n",
    "                model = DenseNet((image_height, image_width), channels, num_classes).to(device)\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "            max_accuracy = 0\n",
    "            writer = SummaryWriter(log_dir=log_dir)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "                accuracy, actual, predicted = test(model, device, test_loader, dataset, model_type, writer, epoch)\n",
    "                max_accuracy = max(accuracy, max_accuracy)\n",
    "                scheduler.step()\n",
    "                if (max_accuracy > 98):\n",
    "                    print('Breaking at epoch {}'.format(epoch))\n",
    "                    break\n",
    "\n",
    "            accuracies[f'{model_type} {dataset}'] = max_accuracy\n",
    "            parameters[f'{model_type} {dataset}'] = count_parameters(model)\n",
    "            expected_vs_actual[f'{model_type} {dataset}'] = (actual, predicted)\n",
    "\n",
    "            torch.save(model.state_dict(), \"{}_{}.pt\".format(model_type, dataset))\n",
    "\n",
    "    return accuracies, parameters, expected_vs_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MLP with MNIST\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.283136\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 2.297636\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.279965\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 2.251291\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.266331\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.262765\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.125573\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 2.091556\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.160460\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 2.114602\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.982369\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 1.908246\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.907685\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 1.900480\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.836185\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.725305\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.851156\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 1.539961\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.215818\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 1.576848\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.133031\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 1.248784\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.000739\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 1.295809\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.994315\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.792231\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.282512\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 1.288724\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.118526\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 1.000098\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.914028\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 1.050954\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.957303\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.934553\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.660316\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.877121\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.977369\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.882836\n",
      "\n",
      "Test set: Average loss: 0.6815, Accuracy: 8166/10000 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.856838\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 1.203632\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.436693\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.817724\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.834466\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.822910\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.427851\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 1.345869\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.819734\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.740202\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.853831\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.437173\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.276425\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.899561\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.969336\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.666183\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.785038\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.645465\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.838633\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.620405\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.032499\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.461758\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.491764\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 1.129265\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.287685\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.670848\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.997785\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.746471\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.862181\n",
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 1.170159\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.639333\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.805034\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.364806\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.561715\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.828182\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.329351\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.675966\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.406813\n",
      "\n",
      "Test set: Average loss: 0.4787, Accuracy: 8719/10000 (87%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.622271\n",
      "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.612099\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.413024\n",
      "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.489623\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.828324\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.658682\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 1.097779\n",
      "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.316453\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.678687\n",
      "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.531289\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.307346\n",
      "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.416800\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.768798\n",
      "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.583416\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.621779\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.459875\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.410924\n",
      "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.382962\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.805358\n",
      "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.478039\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.881504\n",
      "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.906120\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.248639\n",
      "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.839000\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.800324\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.446270\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.703150\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.809287\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.534305\n",
      "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.487650\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.389687\n",
      "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.453870\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.702074\n",
      "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.763298\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.374340\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.569631\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.938578\n",
      "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.547314\n",
      "\n",
      "Test set: Average loss: 0.4096, Accuracy: 8898/10000 (89%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.624021\n",
      "Train Epoch: 4 [1600/60000 (3%)]\tLoss: 0.876292\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.435389\n",
      "Train Epoch: 4 [4800/60000 (8%)]\tLoss: 0.584031\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.242936\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.784532\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.456078\n",
      "Train Epoch: 4 [11200/60000 (19%)]\tLoss: 1.341503\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.610188\n",
      "Train Epoch: 4 [14400/60000 (24%)]\tLoss: 0.332795\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.563406\n",
      "Train Epoch: 4 [17600/60000 (29%)]\tLoss: 0.353031\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.705498\n",
      "Train Epoch: 4 [20800/60000 (35%)]\tLoss: 0.288024\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.394558\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.661206\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.476155\n",
      "Train Epoch: 4 [27200/60000 (45%)]\tLoss: 0.790007\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.744326\n",
      "Train Epoch: 4 [30400/60000 (51%)]\tLoss: 0.818603\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.550885\n",
      "Train Epoch: 4 [33600/60000 (56%)]\tLoss: 0.494180\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.765829\n",
      "Train Epoch: 4 [36800/60000 (61%)]\tLoss: 0.358779\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.478373\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.154499\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.405188\n",
      "Train Epoch: 4 [43200/60000 (72%)]\tLoss: 0.373144\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.770541\n",
      "Train Epoch: 4 [46400/60000 (77%)]\tLoss: 0.295928\n"
     ]
    }
   ],
   "source": [
    "batch_size      = 16             # Input batch size for training (default: 64)\n",
    "epochs          = 20             # Number of epochs to train (default: 20)\n",
    "lr              = .00001         # Learning rate (default: .00001)\n",
    "gamma           = 0.7            # Learning rate step gamma (default: 0.7)\n",
    "test_batch_size = 1000           # Input batch size for testing (default: 1000)\n",
    "log_dir         = \"./logs/tb\"    # TF log dir (default: \"./logs/tb\")\n",
    "log_interval    = 100            # Batch interval for logging (default: 100)\n",
    "\n",
    "accuracies, parameters, expected_vs_actual = run(batch_size, test_batch_size, lr, gamma, epochs, log_dir, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the collected metrics\n",
    "times = Timer.times()\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, figsize=(20,10))\n",
    "bar_colors = ['tab:red', 'tab:green', 'tab:blue', 'tab:orange', 'tab:purple']\n",
    "ax1.bar(list(times.keys()), list(times.values()), color=bar_colors)\n",
    "ax1.set_ylabel('Inference Times (ms)')\n",
    "ax2.bar(list(times.keys()), list(parameters.values()), color=bar_colors)\n",
    "ax2.set_ylabel('Total Parameters')\n",
    "ax3.bar(list(times.keys()), list(accuracies.values()), color=bar_colors)\n",
    "ax3.set_ylabel('Model Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "for key, (actual, predicted) in expected_vs_actual.items():\n",
    "    fig, (ax1) = plt.subplots(ncols=1, figsize=(5, 5))\n",
    "    ax1.set_title(f'{key} Confusion Matrix')\n",
    "    cm = metrics.confusion_matrix(actual, predicted)\n",
    "    ax1.matshow(cm, cmap='binary', interpolation=None, aspect='auto')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
