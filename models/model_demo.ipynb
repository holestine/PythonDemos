{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Different Model Types\n",
    "This notebook compares the performance of MLP, CNN, ViT and Hybrid models on MNIST, CIFAR10 and CIFAR100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from vit_pytorch import ViT\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from timer import Timer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\" Counts the number of trainable parameters\n",
    "\n",
    "    Args:\n",
    "        model (object): A PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of trainable parameters\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to convert grayscale to RGB\n",
    "class GrayToRGB(object):\n",
    "    def __call__(self, image):\n",
    "        return image.repeat(3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPNet(nn.Module):\n",
    "    \"\"\" Creates a fully connected multilayer perceptron. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, num_classes, channels):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (array): The resolution of the image\n",
    "            num_classes (int): The number of output classes\n",
    "            channels (int): The color depth of the image\n",
    "        \"\"\"\n",
    "        super(MLPNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(image_size * channels, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    \"\"\" Creates a convolutional neural network. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, num_classes, channels, ):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (tuple): The height and width of the image\n",
    "            num_classes (int): The number of output classes\n",
    "            channels (int): The color depth of the image\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CNNNet, self).__init__()\n",
    "\n",
    "        (image_height, image_width) = image_size\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        fc_size = self.forward(torch.rand((1, channels, image_height, image_width)), True)\n",
    "\n",
    "        self.fc1 = nn.Linear(fc_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, get_fc_size=False):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "            get_fc_size (bool): Flag that determines the flattened size of the last convolutional layer\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        if get_fc_size:\n",
    "            return x.nelement()\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTNet(nn.Module): \n",
    "    \"\"\" Creates a vision transformer. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_size, patch_size, num_classes, channels):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (array): The resolution of the image\n",
    "            patch_size (int): The size (patch_size x patch_size) of the patches to extract from the image \n",
    "            num_classes (int): The number of output classes\n",
    "            channels (int): The color depth of the image\n",
    "        \"\"\"\n",
    "        HIDDEN_SIZE = 768\n",
    "        DEPTH = 6\n",
    "        HEADS = 12\n",
    "        MLP_DIM = 3072\n",
    "        \n",
    "        super(ViTNet, self).__init__()\n",
    "        \n",
    "        self.model = ViT(\n",
    "                        image_size = image_size,\n",
    "                        patch_size = patch_size,\n",
    "                        num_classes = num_classes,\n",
    "                        dim = HIDDEN_SIZE,\n",
    "                        depth = DEPTH,\n",
    "                        heads = HEADS,\n",
    "                        mlp_dim = MLP_DIM,\n",
    "                        channels = channels,\n",
    "                        dropout = 0.1,\n",
    "                        emb_dropout = 0.1\n",
    "                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        x = self.model(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridNet(nn.Module):\n",
    "    \"\"\" Creates a CNN connected to a ViT. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, channels, num_classes):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (array): The resolution of the image\n",
    "            channels (int): The color depth of the image\n",
    "            num_classes (int): The number of output classes\n",
    "        \"\"\"\n",
    "        HIDDEN_SIZE = 768\n",
    "        DEPTH = 6\n",
    "        HEADS = 12\n",
    "        MLP_DIM = 3072\n",
    "\n",
    "        super(HybridNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        image_width = int((math.sqrt(image_size) - 4) / 2)\n",
    "\n",
    "        self.vit = ViT(\n",
    "                        image_size = image_width ** 2,\n",
    "                        patch_size = int(image_width / 2),\n",
    "                        num_classes = num_classes,\n",
    "                        dim = HIDDEN_SIZE,\n",
    "                        depth = DEPTH,\n",
    "                        heads = HEADS,\n",
    "                        mlp_dim = MLP_DIM,\n",
    "                        channels = 64,\n",
    "                        dropout = 0.1,\n",
    "                        emb_dropout = 0.1\n",
    "                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.vit(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    \"\"\" Creates a DenseNet backbone with a new classification head. Derived from the PyTorch nn.Module class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_height, image_width, channels, num_classes):\n",
    "        \"\"\" Initializes the model\n",
    "\n",
    "        Args:\n",
    "            image_size (array): The resolution of the image\n",
    "            channels (int): The color depth of the image\n",
    "            num_classes (int): The number of output classes\n",
    "        \"\"\"\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        \n",
    "        # Replace the first layer of the DenseNet with a new one that has the correct number of input channels.\n",
    "        # All the other parameters are the same as the original DenseNet.\n",
    "        if channels != 3:\n",
    "\n",
    "            self.densenet.features[0] = nn.Conv2d(in_channels  = channels, \n",
    "                                                  out_channels = self.densenet.features.conv0.out_channels, \n",
    "                                                  kernel_size  = self.densenet.features.conv0.kernel_size, \n",
    "                                                  stride       = self.densenet.features.conv0.stride, \n",
    "                                                  padding      = self.densenet.features.conv0.padding, \n",
    "                                                  bias         = self.densenet.features.conv0.bias)\n",
    "\n",
    "        # Use adaptive pooling to allow for various input sizes\n",
    "        self.densenet.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, num_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Performs inference on an image\n",
    "\n",
    "        Args:\n",
    "            x (array): The image\n",
    "\n",
    "        Returns:\n",
    "            array: The prediction distribution\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.densenet.features(x)\n",
    "        x = self.densenet.classifier(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    \"\"\" Performs a single iteration of training\n",
    "\n",
    "    Args:\n",
    "        model (object): The model to train\n",
    "        device (object): The device that will host the training\n",
    "        train_loader (object): The training dataset\n",
    "        optimizer (object): The optimization algorithm\n",
    "        epoch (int): The current epoch\n",
    "        log_interval (int): The frequency to log data to the screen\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, dataset, model_type, writer, epoch):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (object): The model to test\n",
    "        device (object): The device that will host the testing\n",
    "        test_loader (object): The test dataset\n",
    "        dataset (string): The name of the dataset to use in testing\n",
    "        model_type (string): The type of the model to test\n",
    "        writer (object): Tensorboard logger\n",
    "        epoch (int): The current epoch in training\n",
    "\n",
    "    Returns:\n",
    "        accuracy (): \n",
    "        actual ():\n",
    "        predicted (): \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            with Timer(\"{} {}\".format(model_type, dataset)):\n",
    "                output = model(data)\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            actual.extend(target.cpu().numpy())\n",
    "            predicted.extend([np.argmax(pred) for pred in output.cpu().numpy()])\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    writer.add_scalar('Accuracy/{} {}'.format(model_type, dataset), accuracy, epoch)\n",
    "    return accuracy, actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models and datasets that we'll be evaluating\n",
    "\n",
    "#MODEL_TYPES = [\"MLP\", \"CNN\", \"ViT\", \"HYBRID\", \"DenseNet\"]\n",
    "DATASETS    = [\"MNIST\", \"CIFAR10\", \"CIFAR100\"] \n",
    "MODEL_TYPES = [\"DenseNet\"]\n",
    "#DATASETS    = [\"MNIST\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(batch_size, test_batch_size, lr, gamma, epochs, log_dir, log_interval):\n",
    "    \"\"\" Performs training and testing for all models and datasets specified above \n",
    "\n",
    "    Args:\n",
    "        batch_size (int): The training batch size\n",
    "        test_batch_size (int): The test batch size\n",
    "        lr (float): The learning rate\n",
    "        gamma (float): _description_\n",
    "        epochs (int): The number of epochs of training\n",
    "        log_dir (_type_): The directory for the logs\n",
    "        log_interval (int): How frequently to log to the screen\n",
    "\n",
    "    Returns:\n",
    "        accuracies (dictionary): The model accuracies by model type and dataset\n",
    "        parameters (dictionary): The number of trainable parameters in each model\n",
    "        expected_vs_actual (dictionary): The expected and actual values used in testing each of the models\n",
    "    \"\"\"\n",
    "    # Determine if we should use a GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # Reduce precision to speed up training\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs  = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    accuracies = {}\n",
    "    parameters = {}\n",
    "    expected_vs_actual = {}\n",
    "    \n",
    "    for dataset in DATASETS:\n",
    "        for model_type in MODEL_TYPES:\n",
    "            print(\"Evaluating {} with {}\".format(model_type, dataset))\n",
    "\n",
    "            if dataset == \"MNIST\":\n",
    "                if model_type == \"DenseNet\":\n",
    "                    transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307), (0.3081)),\n",
    "                    transforms.Resize((32, 32)),\n",
    "                    #GrayToRGB(),\n",
    "                ])\n",
    "                else:\n",
    "                    transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307), (0.3081))\n",
    "                ])\n",
    "                train_set = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "                test_set = datasets.MNIST('./data', train=False, transform=transform)\n",
    "                image_height = 28\n",
    "                image_width = 28\n",
    "                patch_size = 14\n",
    "                num_classes = 10\n",
    "                channels = 1\n",
    "            elif dataset == \"CIFAR10\":\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                ])\n",
    "                train_set = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "                test_set = datasets.CIFAR10('./data', train=False, transform=transform)\n",
    "                image_height = 32\n",
    "                image_width = 32\n",
    "                patch_size = 16\n",
    "                num_classes = 10\n",
    "                channels = 3\n",
    "            elif dataset == \"CIFAR100\":\n",
    "                transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                ])\n",
    "                train_set = datasets.CIFAR100('./data', train=True, download=True, transform=transform)\n",
    "                test_set = datasets.CIFAR100('./data', train=False, transform=transform)\n",
    "                image_height = 32\n",
    "                image_width = 32\n",
    "                patch_size = 16\n",
    "                num_classes = 100\n",
    "                channels = 3\n",
    "                \n",
    "            train_loader = torch.utils.data.DataLoader(train_set, **train_kwargs)\n",
    "            test_loader  = torch.utils.data.DataLoader(test_set, **test_kwargs)\n",
    "\n",
    "            if model_type == \"MLP\":\n",
    "                model = MLPNet(image_height * image_width, num_classes, channels).to(device)\n",
    "            elif model_type == \"CNN\":\n",
    "                model = CNNNet((image_height, image_width), num_classes, channels).to(device)\n",
    "            elif model_type == \"ViT\":\n",
    "                model = ViTNet(image_height * image_width, patch_size, num_classes, channels).to(device)\n",
    "            elif model_type == \"HYBRID\":\n",
    "                model = HybridNet(image_height * image_width, channels, num_classes).to(device)\n",
    "            elif model_type == \"DenseNet\":\n",
    "                model = DenseNet(image_height, image_width, channels, num_classes).to(device)\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "            max_accuracy = 0\n",
    "            writer = SummaryWriter(log_dir=log_dir)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "                accuracy, actual, predicted = test(model, device, test_loader, dataset, model_type, writer, epoch)\n",
    "                max_accuracy = max(accuracy, max_accuracy)\n",
    "                scheduler.step()\n",
    "                if (max_accuracy > 98):\n",
    "                    print('Breaking at epoch {}'.format(epoch))\n",
    "                    break\n",
    "\n",
    "            accuracies[f'{model_type} {dataset}'] = max_accuracy\n",
    "            parameters[f'{model_type} {dataset}'] = count_parameters(model)\n",
    "            expected_vs_actual[f'{model_type} {dataset}'] = (actual, predicted)\n",
    "\n",
    "            torch.save(model.state_dict(), \"{}_{}.pt\".format(model_type, dataset))\n",
    "\n",
    "    return accuracies, parameters, expected_vs_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 16             # Input batch size for training (default: 64)\n",
    "epochs          = 1             # Number of epochs to train (default: 20)\n",
    "lr              = .00001         # Learning rate (default: .00001)\n",
    "gamma           = 0.7            # Learning rate step gamma (default: 0.7)\n",
    "test_batch_size = 1000           # Input batch size for testing (default: 1000)\n",
    "log_dir         = \"./logs/tb\"    # TF log dir (default: \"./logs/tb\")\n",
    "log_interval    = 100            # Batch interval for logging (default: 100)\n",
    "\n",
    "accuracies, parameters, expected_vs_actual = run(batch_size, test_batch_size, lr, gamma, epochs, log_dir, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the collected metrics\n",
    "times = Timer.times()\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, figsize=(20,10))\n",
    "bar_colors = ['tab:red', 'tab:green', 'tab:blue', 'tab:orange', 'tab:purple']\n",
    "ax1.bar(list(times.keys()), list(times.values()), color=bar_colors)\n",
    "ax1.set_ylabel('Inference Times (ms)')\n",
    "ax2.bar(list(times.keys()), list(parameters.values()), color=bar_colors)\n",
    "ax2.set_ylabel('Total Parameters')\n",
    "ax3.bar(list(times.keys()), list(accuracies.values()), color=bar_colors)\n",
    "ax3.set_ylabel('Model Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "for key, (actual, predicted) in expected_vs_actual.items():\n",
    "    fig, (ax1) = plt.subplots(ncols=1, figsize=(5, 5))\n",
    "    ax1.set_title(f'{key} Confusion Matrix')\n",
    "    cm = metrics.confusion_matrix(actual, predicted)\n",
    "    ax1.matshow(cm, cmap='binary', interpolation=None, aspect='auto')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
