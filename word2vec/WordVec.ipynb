{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Vectors\n",
    "First let's get all the libraries we're going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before exectuting the following cell download and extract the pretrained word vectors from the file *glove.6B.zip* on the [Stanford GloVe Site](https://github.com/stanfordnlp/GloVe) or use this direct link [glove.6B.zip](http://nlp.stanford.edu/data/wordvecs/glove.6B.zip). Each of the extracted files contains a vocabulary of 400,000 words trained on Wikipedia 2014 data using the word vectors lengths 50, 100, 200 and 300. This notebook uses the 50-dimensional vectors since the computational requirements are lower but better results are expected with the higher dimensionality vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4168020/1959838579.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    }
   ],
   "source": [
    "glove_file = \"./GloVe/glove.6B.50d.txt\"\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.50d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell will probably take some time to execute especially if using the higher dimensionality word vectors but once completed we have a model that we can do a few operations with like getting the words most similar to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('town', 0.8687878251075745),\n",
       " ('downtown', 0.8534142374992371),\n",
       " ('where', 0.8525029420852661),\n",
       " ('cities', 0.8504900932312012),\n",
       " ('area', 0.8322184085845947),\n",
       " ('in', 0.8228286504745483),\n",
       " ('outside', 0.8224184513092041),\n",
       " ('near', 0.8144022822380066),\n",
       " ('central', 0.8133048415184021),\n",
       " ('nearby', 0.7947685122489929)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or getting the words least similar to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jean-noÃ«l', 0.762194812297821),\n",
       " ('lumpkins', 0.755791962146759),\n",
       " ('rw96', 0.7529499530792236),\n",
       " ('muki', 0.752063512802124),\n",
       " ('pead', 0.7512499690055847),\n",
       " ('sonication', 0.7482436299324036),\n",
       " ('linders', 0.748216450214386),\n",
       " ('zakiya', 0.7451493144035339),\n",
       " ('arlow', 0.7435874938964844),\n",
       " ('renie', 0.740894079208374)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(negative='city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide combinations of positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523604273796082),\n",
       " ('throne', 0.7664334177970886),\n",
       " ('prince', 0.7592144012451172),\n",
       " ('daughter', 0.7473883628845215),\n",
       " ('elizabeth', 0.7460219860076904),\n",
       " ('princess', 0.7424570322036743),\n",
       " ('kingdom', 0.7337412238121033),\n",
       " ('monarch', 0.721449077129364),\n",
       " ('eldest', 0.7184861898422241),\n",
       " ('widow', 0.7099431157112122)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to understand this is as an the analogy a is to b as c is to what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def analogy(a, b, c):\n",
    "    return model.most_similar(positive=[c, b], negative=[a])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man', 'king', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'longest'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('tall', 'tallest', 'long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dreadful'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('good', 'fantastic', 'bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aunt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('dad', 'uncle', 'mom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'australian'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('japan', 'japanese', 'australia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'champagne'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('australia', 'beer', 'france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'archaeopteryx'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('human', 'neanderthal', 'bird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tennessee'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('colorado', 'denver', 'oregon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tennessee', 0.8149315714836121),\n",
       " ('indiana', 0.809821605682373),\n",
       " ('baltimore', 0.807337760925293),\n",
       " ('ohio', 0.8069354295730591),\n",
       " ('kansas', 0.8066296577453613),\n",
       " ('jacksonville', 0.8002077341079712),\n",
       " ('michigan', 0.7979021072387695),\n",
       " ('oklahoma', 0.7903385162353516),\n",
       " ('illinois', 0.7901497483253479),\n",
       " ('portland', 0.7884668111801147)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['oregon', 'denver'], negative=['colorado'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what item in a list doesn't match. The odd man out is determined by [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noodles'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match([\"hats\", \"shirt\", \"noodles\", \"socks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the values for the vector associated with each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.62404  ,  0.42443  , -0.18613  , -0.69873  , -0.60087  ,\n",
       "       -0.038885 ,  0.3615   ,  0.32901  ,  0.67713  , -0.08838  ,\n",
       "        0.057528 , -0.82651  ,  0.664    ,  0.2334   ,  0.25474  ,\n",
       "       -0.13098  , -0.97275  , -0.2624   , -0.11682  , -0.0037848,\n",
       "        0.3382   ,  0.5307   ,  0.39923  ,  0.15786  ,  0.29139  ,\n",
       "       -0.90575  , -0.53604  , -0.47801  , -1.308    , -0.67496  ,\n",
       "        1.8521   ,  0.2379   , -0.91865  , -0.80798  ,  1.1039   ,\n",
       "        0.47462  ,  1.5251   ,  1.0385   ,  1.0177   , -0.63719  ,\n",
       "       -0.14075  , -0.56275  , -0.10259  ,  1.6731   , -0.79339  ,\n",
       "       -0.96243  ,  1.7332   ,  1.0047   ,  0.95737  ,  0.045939 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"exam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.4340076"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(model[\"exam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.62143  , -0.49645  , -0.69599  ,  0.19473  ,  1.0616   ,\n",
       "        0.14227  , -0.79524  ,  0.19404  ,  0.0071687,  0.14646  ,\n",
       "       -0.61261  ,  0.0037311,  0.41935  ,  1.0381   ,  0.16911  ,\n",
       "       -0.53342  , -0.52508  ,  0.79629  , -0.029128 , -0.44912  ,\n",
       "        1.0138   , -0.59214  ,  0.17643  ,  1.5506   , -0.96916  ,\n",
       "       -0.42896  , -0.92664  ,  0.43301  ,  1.2915   , -0.80836  ,\n",
       "        1.336    ,  0.24572  , -0.11799  ,  2.0015   , -0.27431  ,\n",
       "        0.17803  , -0.31508  ,  0.84582  ,  0.77419  ,  0.45243  ,\n",
       "        0.73485  ,  0.44473  , -0.77466  ,  0.43471  ,  0.53486  ,\n",
       "        0.98216  , -0.70731  , -0.48557  ,  0.16453  ,  0.65013  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"pizza\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.089899"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(model[\"pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.6100278e-03, -7.2019994e-02, -8.8212001e-01, -5.0400001e-01,\n",
       "        4.6072996e-01,  1.0338500e-01, -4.3373999e-01,  5.2305001e-01,\n",
       "        6.8429869e-01,  5.8079995e-02, -5.5508196e-01, -8.2277894e-01,\n",
       "        1.0833499e+00,  1.2715000e+00,  4.2385000e-01, -6.6440004e-01,\n",
       "       -1.4978300e+00,  5.3389001e-01, -1.4594799e-01, -4.5290479e-01,\n",
       "        1.3520000e+00, -6.1439991e-02,  5.7565999e-01,  1.7084601e+00,\n",
       "       -6.7777002e-01, -1.3347100e+00, -1.4626800e+00, -4.4999987e-02,\n",
       "       -1.6499996e-02, -1.4833200e+00,  3.1880999e+00,  4.8361999e-01,\n",
       "       -1.0366399e+00,  1.1935198e+00,  8.2958996e-01,  6.5265000e-01,\n",
       "        1.2100201e+00,  1.8843200e+00,  1.7918899e+00, -1.8475997e-01,\n",
       "        5.9410000e-01, -1.1801997e-01, -8.7725002e-01,  2.1078100e+00,\n",
       "       -2.5852996e-01,  1.9729972e-02,  1.0258899e+00,  5.1912993e-01,\n",
       "        1.1219000e+00,  6.9606900e-01], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"exam\"] + model[\"pizza\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.210579"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(model[\"exam\"] + model[\"pizza\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Information**\n",
    "- [Stanford CS224N NLP with Deep Learning Material](https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&ab_channel=stanfordonline)\n",
    "- [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [GenSim KeyedVectors API](https://radimrehurek.com/gensim/models/keyedvectors.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
